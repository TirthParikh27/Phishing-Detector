{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipaddress\n",
    "import re\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "import socket\n",
    "import requests\n",
    "from googlesearch import search\n",
    "import whois\n",
    "from datetime import date, datetime\n",
    "import time\n",
    "from dateutil.parser import parse as date_parse\n",
    "from timebudget import timebudget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_row_url(url):\n",
    "    row = []\n",
    "    if not re.match(r\"^https?\", url):\n",
    "        url = \"http://\" + url\n",
    "\n",
    "        \n",
    "    #1 IP address in URL\n",
    "    try:\n",
    "        ip = ipaddress.ip_address(url)\n",
    "        row.append(-1)\n",
    "    except ValueError:\n",
    "        row.append(1)\n",
    "    \n",
    "    #2  Long URL to Hide the Suspicious Part\n",
    "    length = len(url)\n",
    "    if length >= 76:\n",
    "        row.append(-1)\n",
    "    elif length < 55:\n",
    "        row.append(1)\n",
    "    else:\n",
    "        row.append(0)\n",
    "    \n",
    "    #3 Using URL Shortening Services “TinyURL”\n",
    "    # ADD 303 response code\n",
    "    match = re.search('bit\\.ly|goo\\.gl|shorte\\.st|go2l\\.ink|x\\.co|ow\\.ly|t\\.co|tinyurl|tr\\.im|is\\.gd|cli\\.gs|'\n",
    "                      'yfrog\\.com|migre\\.me|ff\\.im|tiny\\.cc|url4\\.eu|twit\\.ac|su\\.pr|twurl\\.nl|snipurl\\.com|'\n",
    "                      'short\\.to|BudURL\\.com|ping\\.fm|post\\.ly|Just\\.as|bkite\\.com|snipr\\.com|fic\\.kr|loopt\\.us|'\n",
    "                      'doiop\\.com|short\\.ie|kl\\.am|wp\\.me|rubyurl\\.com|om\\.ly|to\\.ly|bit\\.do|t\\.co|lnkd\\.in|'\n",
    "                      'db\\.tt|qr\\.ae|adf\\.ly|goo\\.gl|bitly\\.com|cur\\.lv|tinyurl\\.com|ow\\.ly|bit\\.ly|ity\\.im|'\n",
    "                      'q\\.gs|is\\.gd|po\\.st|bc\\.vc|twitthis\\.com|u\\.to|j\\.mp|buzurl\\.com|cutt\\.us|u\\.bb|yourls\\.org|'\n",
    "                      'x\\.co|prettylinkpro\\.com|scrnch\\.me|filoops\\.info|vzturl\\.com|qr\\.net|1url\\.com|tweez\\.me|v\\.gd|tr\\.im|link\\.zip\\.net', url)\n",
    "    if match :\n",
    "        row.append(-1)\n",
    "    else :\n",
    "        row.append(1)\n",
    "    \n",
    "    #4 URL’s having “@” Symbol\n",
    "    if \"@\" in url:\n",
    "        row.append(-1)\n",
    "    else :\n",
    "        row.append(1)\n",
    "    \n",
    "    #5 Redirecting using “//”\n",
    "    index = url.rfind(\"//\")\n",
    "    if index > 6:\n",
    "        row.append(-1)\n",
    "    else:\n",
    "        row.append(1)\n",
    "    \n",
    "    #6 Adding Prefix or Suffix Separated by (-) to the Domain\n",
    "    if re.findall(r\"https?://[^\\-]+-[^\\-]+/\", url):\n",
    "        row.append(-1)\n",
    "    else:\n",
    "        row.append(1)\n",
    "    \n",
    "    #7 Sub Domain and Multi Sub Domains\n",
    "    subdomain_count = len(re.findall(\"/.\" , url))\n",
    "    if subdomain_count == 2:\n",
    "        row.append(0)\n",
    "    elif subdomain_count == 1:\n",
    "        row.append(1)\n",
    "    else:\n",
    "        row.append(-1)\n",
    "    \n",
    "    #8 HTTPS (Hyper Text Transfer Protocol with Secure Sockets Layer) \n",
    "    #print(\"https url : \" , url[:6])\n",
    "    if url[:6] == \"https:\":\n",
    "        row.append(1)\n",
    "    else:\n",
    "        row.append(-1)\n",
    "    \n",
    "    \n",
    "    return row\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def domain_add_row(url , row):\n",
    "    if not re.match(r\"^https?\", url):\n",
    "        url = \"http://\" + url\n",
    "\n",
    "    domain = re.findall(r\"://([^/]+)/?\", url)[0]\n",
    "    if re.match(r\"^www.\", domain):\n",
    "        domain = domain.replace(\"www.\", \"\")\n",
    "   \n",
    "    with timebudget(\"Request \"):\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            res_beauty = BeautifulSoup(response.text, 'html.parser')\n",
    "        except:\n",
    "            response = \"\"\n",
    "            res_beauty = -999\n",
    "    \n",
    "    \n",
    "    #9 Domain Registration Length\n",
    "    with timebudget(\"Whois \"):\n",
    "        try:\n",
    "            whois_response = whois.whois(domain)\n",
    "            expiration_date = whois_response.expiration_date\n",
    "            registration_length = 0\n",
    "            try:\n",
    "                expiration_date = min(expiration_date)\n",
    "                today = time.strftime('%Y-%m-%d')\n",
    "                today = datetime.strptime(today, '%Y-%m-%d')\n",
    "                registration_length = abs((expiration_date - today).days)\n",
    "    #             print(\"HELLLOOOOo\")\n",
    "    #             print(registration_length)\n",
    "                if registration_length / 365 <= 1:\n",
    "                    row.append(-1)\n",
    "                else:\n",
    "                    row.append(1)\n",
    "            except:\n",
    "                row.append(-1)\n",
    "        except:\n",
    "            row.append(-1)\n",
    "    \n",
    "    \n",
    "    #10 Using Non-Standard Port \n",
    "    closed_ports = [21,22,23,445,1433,1521,3306,3389]\n",
    "    flag = 0\n",
    "    try:\n",
    "        port_url = domain.split(\":\")[1]\n",
    "        for port in closed_ports:\n",
    "            if port in port_url:\n",
    "                row.append(-1)\n",
    "                flag = 1\n",
    "                break\n",
    "        if flag == 0:\n",
    "            row.append(1)\n",
    "    \n",
    "    except:\n",
    "        row.append(1)\n",
    "    \n",
    "    #11 HTTPS_token\n",
    "#     print(re.findall(r\"^https://\", url))\n",
    "    if re.findall(r\"^https://\", url):\n",
    "        row.append(1)\n",
    "    else:\n",
    "        row.append(-1)\n",
    "   \n",
    "    return [row , response , res_beauty]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_add_row(url , row_response_soup):\n",
    "    if not re.match(r\"^https?\", url):\n",
    "        url = \"http://\" + url\n",
    "\n",
    "    try:\n",
    "        response = row_response_soup[1]\n",
    "        soup = row_response_soup[2]\n",
    "    except:\n",
    "        response = \"\"\n",
    "        soup = -999\n",
    "\n",
    "    domain = re.findall(r\"://([^/]+)/?\", url)[0]\n",
    "    if re.match(r\"^www.\", domain):\n",
    "        domain = domain.replace(\"www.\", \"\")\n",
    "    \n",
    "    row = row_response_soup[0]\n",
    "    #12 Domain Age \n",
    "    #13 DNS record\n",
    "    if response == \"\":\n",
    "        row.append(-1)\n",
    "    else:\n",
    "        try:\n",
    "            whois_response = whois.whois(domain)\n",
    "            try:\n",
    "                registration_date = re.findall(\n",
    "                        r'Registration Date:</div><div class=\"df-value\">([^<]+)</div>', whois_response.text)[0]\n",
    "                if ((date.today().year - date_parse(registration_date).year) * 12 + date.today().month - date_parse(registration_date).month) >= 6: \n",
    "                    row.append(-1)\n",
    "                else:\n",
    "                    row.append(1)\n",
    "                row.append(1)\n",
    "            except:\n",
    "                row.append(1)\n",
    "                row.append(1)\n",
    "        except:\n",
    "            row.append(-1)\n",
    "            row.append(-1)\n",
    "    \n",
    "    \n",
    "#     #13 DNS Record\n",
    "#     try:\n",
    "#         d = whois.whois(domain)\n",
    "#         row.append(1)\n",
    "#     except:\n",
    "#         row.append(-1)\n",
    "    print(url)\n",
    "    #14 Website Traffic \n",
    "    \n",
    "    rank = BeautifulSoup(requests.get(\n",
    "        \"http://data.alexa.com/data?cli=10&dat=s&url=\" + url).text, \"html.parser\").find(\"reach\")['rank']\n",
    "\n",
    "    rank = int(rank)\n",
    "    print(rank)\n",
    "#         print(rank)\n",
    "    if (rank > 100000):\n",
    "        row.append(0)\n",
    "    else:\n",
    "        row.append(1)\n",
    "\n",
    "#     print(e)\n",
    "#     row.append(-1)\n",
    "    \n",
    "#     #15 PageRank\n",
    "   \n",
    "#     try:\n",
    "#         global_rank = int(re.findall(r\"Global Rank: ([0-9]+)\", rank_checker_response.text)[0])\n",
    "#     except:\n",
    "#         global_rank = -1\n",
    "        \n",
    "#     if global_rank > 0 and global_rank < 100000:\n",
    "#         row.append(-1)\n",
    "#     else:\n",
    "#         row.append(1)\n",
    "\n",
    "    #15 Google Index\n",
    "#     print(search(url,5))\n",
    "    if search(url, 5):\n",
    "        row.append(1)\n",
    "    else:\n",
    "        row.append(-1)\n",
    "    \n",
    "    #16 Statistical_report\n",
    "    url_match = re.search(\n",
    "        'at\\.ua|usa\\.cc|baltazarpresentes\\.com\\.br|pe\\.hu|esy\\.es|hol\\.es|sweddy\\.com|myjino\\.ru|96\\.lt|ow\\.ly', url)\n",
    "    try:\n",
    "        ip_address = socket.gethostbyname(domain)\n",
    "        ip_match = re.search('146\\.112\\.61\\.108|213\\.174\\.157\\.151|121\\.50\\.168\\.88|192\\.185\\.217\\.116|78\\.46\\.211\\.158|181\\.174\\.165\\.13|46\\.242\\.145\\.103|121\\.50\\.168\\.40|83\\.125\\.22\\.219|46\\.242\\.145\\.98|'\n",
    "                             '107\\.151\\.148\\.44|107\\.151\\.148\\.107|64\\.70\\.19\\.203|199\\.184\\.144\\.27|107\\.151\\.148\\.108|107\\.151\\.148\\.109|119\\.28\\.52\\.61|54\\.83\\.43\\.69|52\\.69\\.166\\.231|216\\.58\\.192\\.225|'\n",
    "                             '118\\.184\\.25\\.86|67\\.208\\.74\\.71|23\\.253\\.126\\.58|104\\.239\\.157\\.210|175\\.126\\.123\\.219|141\\.8\\.224\\.221|10\\.10\\.10\\.10|43\\.229\\.108\\.32|103\\.232\\.215\\.140|69\\.172\\.201\\.153|'\n",
    "                             '216\\.218\\.185\\.162|54\\.225\\.104\\.146|103\\.243\\.24\\.98|199\\.59\\.243\\.120|31\\.170\\.160\\.61|213\\.19\\.128\\.77|62\\.113\\.226\\.131|208\\.100\\.26\\.234|195\\.16\\.127\\.102|195\\.16\\.127\\.157|'\n",
    "                             '34\\.196\\.13\\.28|103\\.224\\.212\\.222|172\\.217\\.4\\.225|54\\.72\\.9\\.51|192\\.64\\.147\\.141|198\\.200\\.56\\.183|23\\.253\\.164\\.103|52\\.48\\.191\\.26|52\\.214\\.197\\.72|87\\.98\\.255\\.18|209\\.99\\.17\\.27|'\n",
    "                             '216\\.38\\.62\\.18|104\\.130\\.124\\.96|47\\.89\\.58\\.141|78\\.46\\.211\\.158|54\\.86\\.225\\.156|54\\.82\\.156\\.19|37\\.157\\.192\\.102|204\\.11\\.56\\.48|110\\.34\\.231\\.42', ip_address)\n",
    "        if url_match:\n",
    "            row.append(-1)\n",
    "        elif ip_match:\n",
    "            row.append(-1)\n",
    "        else:\n",
    "            row.append(1)\n",
    "    except:\n",
    "        print('Connection problem. Please check your internet connection')\n",
    "\n",
    "    return row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request  took 938.71ms\n",
      "Whois  took 2.375sec\n",
      "domain_add_row took 3.318sec\n",
      "https://urlth.me/eA7a3\n",
      "1957739\n",
      "rank_add_row took 1.525sec\n",
      "Total time took 4.847sec\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1, 1, 1, 0, 1, -1, 1, 1, 1, 1, 0, 1, 1]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with timebudget(\"Total time\"):\n",
    "    url = \"https://urlth.me/eA7a3\"\n",
    "    row = add_row_url(url)\n",
    "    with timebudget(\"domain_add_row\"):\n",
    "        row2 = domain_add_row(url , row)\n",
    "    with timebudget(\"rank_add_row\"):\n",
    "        row3 = rank_add_row(url , row2)\n",
    "row3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer\n",
    "def extract_tokens(url):\n",
    "    tokens = re.split('[/-]' , url)\n",
    "    \n",
    "    for token in tokens:\n",
    "        if \".\" in token:\n",
    "            final_split = token.split(\".\")\n",
    "            \n",
    "            if \"com\" in final_split:\n",
    "                final_split.remove(\"com\")\n",
    "            if \"www\" in final_split:\n",
    "                final_split.remove(\"www\")\n",
    "            \n",
    "            \n",
    "            tokens = tokens + final_split\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
